{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e4d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Latest notebook visualizing results for Distribution Paper by Julia Geller\n",
    "##Updated:  07/02/2022\n",
    "\n",
    "# Required to access the database\n",
    "import os\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Data analysis tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Models available in our application\n",
    "from datasets.models import RawFlower, RawUNM, RawDAR, RawNHANES_BIO\n",
    "from django.contrib.auth.models import User\n",
    "from datasets.models import RawDictionary\n",
    "\n",
    "\n",
    "from datasets.models import RawNEU\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels\n",
    "\n",
    "from api import adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5cf5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "##merges 4 cohorts based on columns in common\n",
    "def merge4CohortFrames2(df1, df2, df3,df4):\n",
    "    'merge on feature intersections'\n",
    "\n",
    "    for as_feature in ['UASB', 'UDMA', 'UAS5', 'UIAS', 'UAS3', 'UMMA']:\n",
    "        if as_feature not in df1.columns:\n",
    "            df1[as_feature] = np.nan\n",
    "        if as_feature not in df2.columns:\n",
    "            df2[as_feature] = np.nan\n",
    "        if as_feature not in df3.columns:\n",
    "            df3[as_feature] = np.nan\n",
    "\n",
    "    s1 = set(df1.columns)\n",
    "    s2 = set(df2.columns)\n",
    "    s3 = set(df3.columns)\n",
    "    s4 = set(df4.columns)\n",
    "\n",
    "    cc = set.intersection(s1, s2, s3,s4)\n",
    "\n",
    "    df_all = pd.concat([df1[cc],df2[cc],df3[cc], df4[cc]])\n",
    "\n",
    "    return (df_all, cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb61732",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Stats\n",
      "PROTECT\n",
      "(570, 32)\n",
      "Data Stats\n",
      "NHBCS\n",
      "(2152, 198)\n",
      "Data Stats\n",
      "Navajo\n",
      "(521, 30)\n",
      "Data Stats\n",
      "NHANES\n",
      "(8583, 23)\n",
      "Data Stats\n",
      "ALL\n",
      "(11826, 16)\n"
     ]
    }
   ],
   "source": [
    "##Create aliases\n",
    "NEU_alias = 'PROTECT'\n",
    "DAR_alias = 'NHBCS'\n",
    "UNM_alias = 'Navajo'\n",
    "NHANES_alias = 'NHANES' \n",
    "## Get the data\n",
    "\n",
    "## NEU (Cohort 1)\n",
    "df_NEU = adapters.neu.get_dataframe_orig()\n",
    "df_NEU = df_NEU[df_NEU['TimePeriod']==2] # Visit 2\n",
    "\n",
    "NEU_covars = adapters.neu.get_dataframe_covars()\n",
    "df_NEU_covars = NEU_covars.merge(df_NEU, on = ['PIN_Patient','CohortType','TimePeriod']) #Merge the covariates\n",
    "df_NEU_covars['CohortType'] = NEU_alias\n",
    "df_NEU['CohortType'] = NEU_alias\n",
    "\n",
    "\n",
    "df_NEU_blod = adapters.neu.get_dataframe_BLOD()\n",
    "df_NEU_blod['CohortType'] = NEU_alias\n",
    "df_NEU_blod = df_NEU_blod[df_NEU_blod['TimePeriod']==2]\n",
    "\n",
    "\n",
    "##DAR (Cohort 2)\n",
    "\n",
    "df_DAR = adapters.dar.get_dataframe()\n",
    "df_DAR['CohortType'] == DAR_alias\n",
    "\n",
    "df_DAR_blod = adapters.dar.get_dataframe_BLOD()\n",
    "df_DAR_blod['CohortType'] = DAR_alias\n",
    "\n",
    "''''DAR_covars = adapters.dar.get_dataframe_covars()\n",
    "df_DAR_covars = DAR_covars.merge(df_DAR, on = ['PIN_Patient','CohortType','TimePeriod']) #Merge the covariates\n",
    "'''\n",
    "df_DAR['CohortType'] = DAR_alias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_UNM = adapters.unm.get_dataframe_orig()\n",
    "df_UNM['CohortType'] == UNM_alias\n",
    "\n",
    "df_UNM_blod = adapters.unm.get_dataframe_BLOD()\n",
    "df_UNM_blod['CohortType'] = UNM_alias\n",
    "\n",
    "UNM_covars = adapters.unm.get_dataframe_covars()\n",
    "df_UNM_covars = UNM_covars.merge(df_UNM, on = ['PIN_Patient','CohortType','TimePeriod']) #Merge the covariates\n",
    "df_UNM_covars['CohortType'] = UNM_alias\n",
    "'''\n",
    "\n",
    "##!!DELETE FOR WHEN FINALIZING RESULTS \n",
    "df_UNM = df_NEU.copy()\n",
    "df_UNM['CohortType'] = UNM_alias\n",
    "df_UNM_blod = df_NEU_blod.copy()\n",
    "df_UNM_blod['CohortType'] = UNM_alias\n",
    "df_UNM_covars = df_NEU_covars.copy()\n",
    "df_UNM_covars['CohortType'] = UNM_alias\n",
    "\n",
    "'''\n",
    "\n",
    "##NHANES \n",
    "\n",
    "df_NHANES = adapters.nhanes.get_dataframe_orig()\n",
    "df_NHANES = df_NHANES.rename_axis(None, axis=1)\n",
    "\n",
    "df_NHANES_blod = adapters.nhanes.get_dataframe_orig_blod()\n",
    "\n",
    "NHANES_covars = adapters.nhanes.get_dataframe_covars()\n",
    "NHANES_covars['CohortType'] = NHANES_alias\n",
    "\n",
    "##df_NHANES_covars = NHANES_covars.merge(df_NHANES, on = ['PIN_Patient','CohortType','TimePeriod'])\n",
    "df_NHANES['CohortType'] = NHANES_alias\n",
    "df_NHANES_blod['CohortType'] = NHANES_alias\n",
    "\n",
    "df_ALL, intersec_cols = merge4CohortFrames2(df_NEU, df_UNM, df_DAR, df_NHANES)\n",
    "\n",
    "\n",
    "## df_ALL = analysis.merge3CohortFrames(df_NEU, df_UNM, df_DAR)\n",
    "frames_for_analysis = [\n",
    "    (NEU_alias, df_NEU),\n",
    "    (DAR_alias, df_DAR),\n",
    "    (UNM_alias, df_UNM),\n",
    "    (NHANES_alias, df_NHANES),\n",
    "    ('ALL', df_ALL)\n",
    "\n",
    "]\n",
    "\n",
    "for name, df in frames_for_analysis:\n",
    "    print('Data Stats')\n",
    "    print(name)\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f68e4ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8744 entries, 0 to 8743\n",
      "Data columns (total 23 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PIN_Patient  8744 non-null   int64  \n",
      " 1   TimePeriod   8744 non-null   object \n",
      " 2   UALB_mg      6376 non-null   float64\n",
      " 3   UALB_ug      6376 non-null   float64\n",
      " 4   UBA          2893 non-null   float64\n",
      " 5   UCD          2893 non-null   float64\n",
      " 6   UCO          2893 non-null   float64\n",
      " 7   UCR          621 non-null    float64\n",
      " 8   UCRT_mg      4055 non-null   float64\n",
      " 9   UCRT_umol    4055 non-null   float64\n",
      " 10  UCS          2893 non-null   float64\n",
      " 11  UHG          2895 non-null   float64\n",
      " 12  UI           1354 non-null   float64\n",
      " 13  UMN          2893 non-null   float64\n",
      " 14  UMO          2892 non-null   float64\n",
      " 15  UNI          621 non-null    float64\n",
      " 16  UPB          2893 non-null   float64\n",
      " 17  USB          2893 non-null   float64\n",
      " 18  USN          2892 non-null   float64\n",
      " 19  UTAS         1376 non-null   float64\n",
      " 20  UTL          2893 non-null   float64\n",
      " 21  UTU          2887 non-null   float64\n",
      " 22  CohortType   8744 non-null   object \n",
      "dtypes: float64(20), int64(1), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_NHANES_blod.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b35ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytes in common acrosss all 4 cohorts\n",
    "intersec_analytes = list(intersec_cols)\n",
    "intersec_analytes.remove('CohortType')\n",
    "intersec_analytes.remove('PIN_Patient')\n",
    "intersec_analytes.remove('TimePeriod')\n",
    "intersec_analytes\n",
    "\n",
    "# Analyte adjustments\n",
    "def adjust(neu, unm, dar, nhanes, cols):\n",
    "\n",
    "    for conc_var in cols:\n",
    "        dar[conc_var] = dar[conc_var] * (np.nanmedian(dar['urine_specific_gravity']) - 1) / (dar['urine_specific_gravity']-1)\n",
    "\n",
    "    for conc_var in cols:\n",
    "        unm[conc_var] = unm[conc_var] * (np.nanmedian(unm['creatininemgdl']) - 1) / (unm['creatininemgdl']-1)\n",
    "        #unm[conc_var] = unm[conc_var] * (np.nanmedian(unm['urine_specific_gravity']) - 1) / (unm['urine_specific_gravity']-1)\n",
    "    \n",
    "    for conc_var in cols:\n",
    "        neu[conc_var] = neu[conc_var] * (np.nanmedian(neu['SPECIFICGRAVITY_V2']) - 1) / (neu['SPECIFICGRAVITY_V2']-1)\n",
    "\n",
    "    for conc_var in cols:\n",
    "        nhanes[conc_var] = nhanes[conc_var] * (np.nanmedian(nhanes['UCRT_mg']) - 1) / (nhanes['UCRT_mg']-1)\n",
    "        \n",
    "    return neu, unm, dar, nhanes\n",
    "\n",
    "df_NEU, df_UNM, df_DAR, df_NHANES = adjust(df_NEU_covars, df_UNM_covars, df_DAR, df_NHANES, intersec_analytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc80cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [(df_NEU,df_NEU_blod), (df_UNM, df_UNM_blod), (df_DAR, df_DAR_blod), (df_NHANES, df_NHANES_blod)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b502a0ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_above_lod_df(dfs, intersec_cols, dar_alias):\n",
    "    flag = False\n",
    "    res = [df_NEU.copy(), df_UNM.copy(), df_DAR.copy(), df_NHANES.copy()]\n",
    "    for j in range(0, 4):\n",
    "        # Creating copy of NEU df where all blod values are NaN\n",
    "        df_res = res[j]\n",
    "        blod_id_col = 'PIN_Patient'\n",
    "        for i in range(0, len(df_res)):\n",
    "            if df_res['CohortType'].unique() == dar_alias:\n",
    "                flag = True\n",
    "                blod_id_col = 'unq_id'\n",
    "            row = df_res.iloc[i]\n",
    "            for a in intersec_cols:\n",
    "                blod_df = dfs[j][1]\n",
    "                blod_row = blod_df[blod_df[blod_id_col] == row['PIN_Patient']]\n",
    "                if blod_row[a].values[0] == 0.0:\n",
    "                    if flag:\n",
    "                        df_res.at[i, a+'_BLOD'] = np.NaN\n",
    "                    df_res.at[i, a] = np.NaN\n",
    "        return res[0], res[1], res[2], res[3]\n",
    "dfs = [(df_NEU,df_NEU_blod), (df_UNM, df_UNM_blod), (df_DAR, df_DAR_blod), (df_NHANES, df_NHANES_blod)]\n",
    "\n",
    "df_NEU_alod, df_UNM_alod, df_DAR_alod, df_NHANES_alod = make_above_lod_df(dfs, intersec_analytes, DAR_alias)\n",
    "df_ALL_alod, intersec_cols_alod = merge4CohortFrames2(df_NEU_alod, df_UNM_alod, df_DAR_alod, df_NHANES_alod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987eef4",
   "metadata": {},
   "source": [
    "**Summary Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "800bf332",
   "metadata": {},
   "outputs": [],
   "source": [
    "##limit summary statistic values to two decimal places\n",
    "def clean_cols(desc_DF):\n",
    "    desc_DF[\"count\"]=desc_DF[\"count\"].astype(int)\n",
    "    desc_DF[\"mean\"]=round(desc_DF[\"mean\"],2)\n",
    "    desc_DF[\"std\"]=round(desc_DF[\"std\"],2)\n",
    "    desc_DF[\"min\"]=round(desc_DF[\"min\"],2)\n",
    "    desc_DF[\"25%\"]=round(desc_DF[\"25%\"],2)\n",
    "    desc_DF[\"50%\"]=round(desc_DF[\"50%\"],2)\n",
    "    desc_DF[\"75%\"]=round(desc_DF[\"75%\"],2)\n",
    "    desc_DF[\"max\"]=round(desc_DF[\"max\"],2)\n",
    "\n",
    "def desc_4_cohs(df_neu, df_dar, df_unm, df_nhanes, fi_name):\n",
    "    ##columns to describe\n",
    "    desc_cols = []\n",
    "    for col in df_ALL.columns:\n",
    "        if col not in ['PIN_Patient', 'TimePeriod', 'CohortType']:\n",
    "            desc_cols.append(col)\n",
    "    # NEU\n",
    "    desc_neu = df_neu[desc_cols].describe().transpose()\n",
    "    clean_cols(desc_neu)\n",
    "\n",
    "    # DAR \n",
    "    desc_dar = df_dar[desc_cols].describe().transpose()\n",
    "    clean_cols(desc_dar)\n",
    "\n",
    "    # UNM\n",
    "    desc_unm = df_unm[desc_cols].describe().transpose()\n",
    "    clean_cols(desc_unm)\n",
    "\n",
    "    # NHANES\n",
    "    desc_nhanes = df_nhanes[desc_cols].describe().transpose()\n",
    "    clean_cols(desc_nhanes)\n",
    "\n",
    "    #Saving each summary dataframe to a csv file.\n",
    "    frames_names = [(desc_neu,NEU_alias), (desc_dar, DAR_alias),(desc_unm, UNM_alias), (desc_nhanes,NHANES_alias)]\n",
    "\n",
    "\n",
    "    try:\n",
    "        f = open(fi_name+\".csv\", \"x\")\n",
    "        f = open(fi_name+\".csv\", \"a\")\n",
    "\n",
    "    except:\n",
    "        f = open(fi_name+\".csv\", \"a\")\n",
    "\n",
    "    for tup in frames_names:\n",
    "        frame=tup[0]\n",
    "        latex_frame=tup[0].to_latex()\n",
    "        name=tup[1]\n",
    "        ##content=string(name ,\"/n\",frame,\"\\n\\n\")\n",
    "        f.write(name + ',')\n",
    "        f.write(\"\\n\\t\")\n",
    "        f.write(str(frame) + ',')\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(latex_frame)\n",
    "\n",
    "    f.close()\n",
    "    return frames_names, desc_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7ad761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_names, desc_cols = desc_4_cohs(df_NEU, df_DAR, df_UNM, df_NHANES, \"Summary_Stats_DP_Paper_JGeller\")\n",
    "frames_names_alod, desc_cols_alod = desc_4_cohs(df_NEU_alod, df_DAR_alod, df_UNM_alod, df_NHANES_alod, \"ALOD_Summary_Stats_DP_Paper_JGeller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5234569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_log_cols(df, intersec):\n",
    "    df_res = df.copy()\n",
    "    for col in intersec:\n",
    "        if col not in ['CohortType','PIN_Patient', 'TimePeriod']:\n",
    "            df_res[col] = np.log10(df[col])\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "583bd684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_NEU_log = df_to_log_cols(df_NEU, intersec_cols)\n",
    "df_DAR_log = df_to_log_cols(df_DAR, intersec_cols)\n",
    "df_UNM_log = df_to_log_cols(df_UNM, intersec_cols)\n",
    "df_NHANES_log = df_to_log_cols(df_NHANES, intersec_cols)\n",
    "\n",
    "df_NEU_log_alod = df_to_log_cols(df_NEU_alod, intersec_cols)\n",
    "df_DAR_log_alod = df_to_log_cols(df_DAR_alod, intersec_cols)\n",
    "df_UNM_log_alod = df_to_log_cols(df_UNM_alod, intersec_cols)\n",
    "df_NHANES_log_alod = df_to_log_cols(df_NHANES_alod, intersec_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8a659c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_names_log, desc_cols_log = desc_4_cohs(df_NEU_log, df_DAR_log, df_UNM_log, df_NHANES_log, 'Summary_Stats_Log_Values_DP_Paper_JGeller')\n",
    "frames_names_log_alod, desc_cols_log_alod = desc_4_cohs(df_NEU_log_alod, df_DAR_log_alod, df_UNM_log_alod, df_NHANES_log_alod, 'ALOD_Summary_Stats_Log_Values_DP_Paper_JGeller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47c7cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ALL_log, intersec_cols_log = merge4CohortFrames2(df_NEU_log, df_UNM_log, df_DAR_log, df_NHANES_log)\n",
    "df_ALL_log_alod, intersec_cols_log_alod = merge4CohortFrames2(df_NEU_log_alod, df_UNM_log_alod, df_DAR_log_alod, df_NHANES_log_alod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b9ebd",
   "metadata": {},
   "source": [
    "**Reporting of the counts per analyte provided by each cohort**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f75d40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##returns a dict with count per analyte\n",
    "def count_dict(frame_name):\n",
    "    ##build dictionary with values coressponding to each column\n",
    "    vals = {}\n",
    "    desc_cols.append('CohortType')\n",
    "    for col in desc_cols:\n",
    "        vals[col] = []\n",
    "\n",
    "    ##report ocunts for each data frame and add values to dictionary\n",
    "    for tup in frame_name:\n",
    "        i = 0\n",
    "        name=tup[1]\n",
    "        vals[\"CohortType\"].append(name)\n",
    "\n",
    "        while i < len(tup[0]):\n",
    "            feature = tup[0].index[i]\n",
    "            feature_count = tup[0][\"count\"][i]\n",
    "            i = i + 1\n",
    "            vals[feature].append(feature_count)\n",
    "\n",
    "    ##create dataframe from values in dictionary\n",
    "    return pd.DataFrame(columns = list(vals.keys()), data = vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295bdd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counts = count_dict(frames_names)\n",
    "counts_alod = count_dict(frames_names_alod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634706bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Displaying counts in a dataframe with a gradient based on value\n",
    "import seaborn as sns\n",
    "cm = sns.light_palette(\"blue\", as_cmap = True)\n",
    "counts_df= counts.style.background_gradient(cmap = cm)\n",
    "counts_df\n",
    "\n",
    "##TAKE SCREENSHOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855fc52",
   "metadata": {},
   "source": [
    "**Making a Histogram of Counts per Analyte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8cfa9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##creates a seaborn histogram based on a melted df and color scheme\n",
    "def hist(df_melted, list_colors, fi_name):\n",
    "    sns.set_style('ticks')\n",
    "    sns.set(font_scale=1.25)\n",
    "    \n",
    "    g = sns.catplot(\n",
    "        data = df_melted,\n",
    "        x = 'CohortType', y = 'value',\n",
    "        col ='variable', kind = 'bar', col_wrap = 5, sharey = False, palette = list_colors\n",
    "    )\n",
    "    # iterate through axes and set bar label as number of datapoints\n",
    "    for ax in g.axes.ravel():\n",
    "\n",
    "        # add annotations\n",
    "        for c in ax.containers:\n",
    "            labels = [f'{(v.get_height()):.0f}' for v in c]\n",
    "            ax.bar_label(c, labels=labels, label_type = 'edge')\n",
    "        ax.margins(y = 0.2)\n",
    "\n",
    "    g.savefig(fi_name, format ='jpeg', dpi =200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_melted = pd.melt(counts, id_vars = ['CohortType'])\n",
    "counts_melted_alod = pd.melt(counts_alod, id_vars = ['CohortType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_var(x):\n",
    "    if x == 'UPB':\n",
    "        return 'Lead'\n",
    "    elif x == 'UMO':\n",
    "        return 'Molybdenum'\n",
    "    elif x =='UCO':\n",
    "        return 'Cobalt'\n",
    "    elif x =='UCS':\n",
    "        return 'Cesium'\n",
    "    elif x =='UHG':\n",
    "        return 'Mercury'\n",
    "    \n",
    "    elif x =='UCD':\n",
    "        return 'Cadmium'\n",
    "    elif x =='UTL':\n",
    "        return 'Thallium'\n",
    "    elif x =='USB':\n",
    "        return 'Antimony'\n",
    "    elif x =='UTAS':\n",
    "        return 'Tin'\n",
    "    elif x =='UBA':\n",
    "        return 'Barium'\n",
    "    elif x =='UMN':\n",
    "        return 'Manganese'\n",
    "    elif x =='UTU':\n",
    "        return 'Tungsten'\n",
    "    else:\n",
    "        return 'NA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO some columns in BLOD have more values than 1.0, or 0.0 (ex df_NEU_bloc['UCD'] has 0.5 as value)\n",
    "df_NEU_blod['UCD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ed5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_melted['variable'] = counts_melted['variable'].apply(rename_var)\n",
    "counts_melted_alod['variable'] = counts_melted_alod['variable'].apply(rename_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5d263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: inccorect numbers for ALOD, protext is almost 0 for alod and rest of cohorts are identical numbers\n",
    "# ALOD vs normal df\n",
    "hist(counts_melted,  ['red', 'green', 'gray', 'blue'], 'Analyte_Counts_Hist_DP_Paper.jpg')\n",
    "hist(counts_melted_alod,  ['red', 'green', 'gray', 'blue'], 'ALOD_Analyte_Counts_Hist_DP_Paper.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8ae96",
   "metadata": {},
   "source": [
    "**Creating LOD Ratio graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a37be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## returns if string is substring in any element of list\n",
    "def list_contains(sub, lst):\n",
    "    answer = False\n",
    "    for col in lst:\n",
    "        answer = answer or (col in lst)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc11fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##round the columns of the df to specified number of digits\n",
    "def round_cols_float(df, dec_places):\n",
    "    df_temp =df.copy()\n",
    "    for col in df_temp.columns:\n",
    "        if col not in ['PIN_Patient', 'TimePeriod', 'CohortType']:\n",
    "            df_temp[col] =df_temp[col].apply(lambda x: round(x, dec_places))\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##helper to build df for lod\n",
    "def lod_helper(df, intersec, coh_name, cohs_acc, var_acc, val_acc, denom_acc, col_ending):\n",
    "    for col in intersec:\n",
    "        if col +col_ending in df.columns:\n",
    "            cohs_acc.append(coh_name)\n",
    "            var_acc.append(col )\n",
    "            val_acc.append(len(df[df[col + col_ending] == 0.0][col +col_ending]))\n",
    "            denom_acc.append(len(df[col+col_ending]) - sum(df[col + col_ending].isna()))\n",
    "    return cohs_acc, var_acc, val_acc, denom_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cohs = []\n",
    "var = []\n",
    "val = []\n",
    "\n",
    "cohs_acc, var_acc, val_acc, denom_acc = lod_helper(df_NEU_blod, intersec_cols, NEU_alias, [], [], [], [], '' )\n",
    "cohs_acc, var_acc, val_acc, denom_acc = lod_helper(df_DAR_blod, intersec_cols, DAR_alias, cohs_acc, var_acc, val_acc, denom_acc, '_BLOD')\n",
    "cohs_acc, var_acc, val_acc, denom_acc = lod_helper(df_UNM_blod, intersec_cols, UNM_alias, cohs_acc, var_acc, val_acc, denom_acc, '')\n",
    "df_NHANES_blod_float = round_cols_float(df_NHANES_blod, 2)\n",
    "\n",
    "\n",
    "cohs_acc, var_acc, val_acc, denom_acc = lod_helper(df_NHANES_blod_float, intersec_cols, NHANES_alias, cohs_acc, var_acc, val_acc, denom_acc, '')\n",
    "\n",
    "\n",
    "blod_counts_melted ={'CohortType':  cohs_acc, 'variable': var_acc, 'value': val_acc, 'N' : denom_acc}\n",
    "blod_df = pd.DataFrame(data = blod_counts_melted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##modifying blod_df to be percent of 0.0 values over all non-na values\n",
    "blod_df_ratio = blod_df.copy()\n",
    "blod_df_ratio['value'] = round(blod_df_ratio['value']/blod_df_ratio['N'],2)\n",
    "m_blod_df_ratio = blod_df_ratio[~blod_df_ratio['variable'].isin(['CohortType', 'PIN_Patient', 'TimePeriod'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc3802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_blod_df_ratio['variable']=m_blod_df_ratio['variable'].apply(rename_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6a25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "\n",
    "g = sns.catplot(\n",
    "    data = m_blod_df_ratio,\n",
    "    x = 'CohortType', y = 'value',\n",
    "    col ='variable', kind = 'bar', col_wrap = 5, sharey = False, palette = ['red', 'green', 'gray', 'blue']\n",
    ")\n",
    "# iterate through axes and set bar label as number of datapoints\n",
    "for ax in g.axes.ravel():\n",
    "\n",
    "    # add annotations\n",
    "    for c in ax.containers:\n",
    "        labels = [f'{(v.get_height()):.2f}' for v in c]\n",
    "        ax.bar_label(c, labels=labels, label_type = 'edge')\n",
    "    ax.margins(y = 0.2)\n",
    "\n",
    "g.savefig('Above_LOD_Ratio_Hist_DP_Paper.jpg', format ='jpeg', dpi =1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d193038",
   "metadata": {},
   "source": [
    "**Creating Graph of Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3fcfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##creating dataframe to create a boxplot\n",
    "def long_on_analyte(df_neu, df_dar, df_unm, df_nhanes, intersec_cols):\n",
    "    cols = intersec_cols.copy()\n",
    "    cols.remove('PIN_Patient')\n",
    "    cols.remove('TimePeriod')\n",
    "    neu_ints_melt = pd.melt(df_neu[cols], id_vars = \"CohortType\")\n",
    "    dar_ints_melt = pd.melt(df_dar[cols], id_vars = \"CohortType\")\n",
    "    unm_ints_melt = pd.melt(df_unm[cols], id_vars = \"CohortType\")\n",
    "    nhanes_ints_melt = pd.melt(df_nhanes[cols], id_vars = \"CohortType\")\n",
    "    return pd.concat([neu_ints_melt, dar_ints_melt, unm_ints_melt, nhanes_ints_melt], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac54659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_combined_stacks_melts = long_on_analyte(df_NEU, df_DAR, df_UNM, df_NHANES, intersec_cols)\n",
    "all_combined_stacks_melts_alod = long_on_analyte(df_NEU_alod, df_DAR_alod, df_UNM_alod, df_NHANES_alod, intersec_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbcdce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "i =0\n",
    "while i < len(all_combined_stacks_melts):\n",
    "    if type(all_combined_stacks_melts.iloc[i]['variable']) != str:\n",
    "        all_combined_stacks_melts.at[i, 'variable'] = all_combined_stacks_melts.iloc[i]['Analyte'] \n",
    "    i = i+1\n",
    "m_all_combined_stacks_melts = all_combined_stacks_melts[['CohortType', 'value', 'variable']]\n",
    "\n",
    "import math\n",
    "i =0\n",
    "while i < len(all_combined_stacks_melts_alod):\n",
    "    if type(all_combined_stacks_melts_alod.iloc[i]['variable']) != str:\n",
    "        all_combined_stacks_melts_alod.at[i, 'variable'] = all_combined_stacks_melts_alod.iloc[i]['Analyte'] \n",
    "    i = i+1\n",
    "m_all_combined_stacks_melts_alod = all_combined_stacks_melts_alod[['CohortType', 'value', 'variable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_all_combined_stacks_melts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_all_combined_stacks_melts_alod.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4bedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_all_combined_stacks_melts['variable']=m_all_combined_stacks_melts['variable'].apply(rename_var)\n",
    "m_all_combined_stacks_melts_alod['variable']=m_all_combined_stacks_melts_alod['variable'].apply(rename_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35753c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_combined_stacks_melts_log = all_combined_stacks_melts.copy()\n",
    "all_combined_stacks_melts_log = long_on_analyte(df_NEU_log, df_DAR_log, df_UNM_log, df_NHANES_log, intersec_cols)\n",
    "all_combined_stacks_melts_log['variable']=all_combined_stacks_melts_log['variable'].apply(rename_var)\n",
    "\n",
    "import math\n",
    "i =0\n",
    "while i < len(all_combined_stacks_melts_log):\n",
    "    if type(all_combined_stacks_melts_log.iloc[i]['variable']) != str:\n",
    "        all_combined_stacks_melts_log.at[i, 'variable'] = all_combined_stacks_melts_log.iloc[i]['Analyte'] \n",
    "    i = i+1\n",
    "m_all_combined_stacks_melts_log = all_combined_stacks_melts_log[['CohortType', 'value', 'variable']]\n",
    "\n",
    "# Distribution Plot on log analyte value\n",
    "\n",
    "g = sns.FacetGrid(m_all_combined_stacks_melts_log, col = 'variable', hue = 'CohortType', col_wrap = 4, sharex = False, sharey = False, palette =  ['red', 'green', 'gray', 'blue'])\n",
    "p1 = g.map(sns.kdeplot, 'value').add_legend()\n",
    "p1.savefig('Dist_Plot_Log_Analyte_DP_Paper.jpg', format = 'jpeg', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#above limit of detection version\n",
    "\n",
    "all_combined_stacks_melts_log_alod = all_combined_stacks_melts_alod.copy()\n",
    "all_combined_stacks_melts_log_alod = long_on_analyte(df_NEU_log_alod, df_DAR_log_alod, df_UNM_log_alod, df_NHANES_log_alod, intersec_cols)\n",
    "all_combined_stacks_melts_log_alod['variable']=all_combined_stacks_melts_log_alod['variable'].apply(rename_var)\n",
    "\n",
    "import math\n",
    "i =0\n",
    "while i < len(all_combined_stacks_melts_log_alod):\n",
    "    if type(all_combined_stacks_melts_log_alod.iloc[i]['variable']) != str:\n",
    "        all_combined_stacks_melts_log_alod.at[i, 'variable'] = all_combined_stacks_melts_log_alod.iloc[i]['Analyte'] \n",
    "    i = i+1\n",
    "m_all_combined_stacks_melts_log_alod = all_combined_stacks_melts_log_alod[['CohortType', 'value', 'variable']]\n",
    "\n",
    "# Distribution Plot on log analyte value\n",
    "\n",
    "g = sns.FacetGrid(m_all_combined_stacks_melts_log_alod, col = 'variable', hue = 'CohortType', col_wrap = 4, sharex = False, sharey = False, palette =  ['red', 'green', 'gray', 'blue'])\n",
    "p1 = g.map(sns.kdeplot, 'value').add_legend()\n",
    "p1.savefig('ALOD_Dist_Plot_Log_Analyte_DP_Paper.jpg', format = 'jpeg', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8e43f",
   "metadata": {},
   "source": [
    "**LOD: Percent of Detects that Fall in Max-Min LOD per Analyte**\n",
    "\n",
    "In this section, a graph is produced that displays the percent of lod's that fall into the range of the max-min lod per detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f691f7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lods = pd.read_csv('LOD_ranges.csv')\n",
    "lods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_analytes = m_all_combined_stacks_melts.copy()\n",
    "all_analytes = all_analytes[all_analytes['variable'].isin(desc_cols)]\n",
    "all_analytes = all_analytes[all_analytes['variable'].isin(lods['Analyte'].values)]\n",
    "all_analytes = all_analytes.reset_index(drop = True)\n",
    "all_analytes['In_LOD_Range'] = 0\n",
    "##for i = range(0, len(m_all_combined_stacks_melts)-1, 1):\n",
    "i = 0\n",
    "while i <len(all_analytes):\n",
    "    var = all_analytes.iloc[i]['variable']\n",
    "    value = all_analytes.iloc[i]['value']\n",
    "    var_min_lod = lods[lods['Analyte']==var]['Min_LOD'].values[0]\n",
    "    var_max_lod = lods[lods['Analyte']==var]['Max_LOD'].values[0]\n",
    "    if value>=var_min_lod and value<=var_max_lod:\n",
    "        all_analytes.at[i, 'In_LOD_Range']=1\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42303385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coh = []\n",
    "analyte = []\n",
    "in_range = []\n",
    "n = []\n",
    "cohorts = [NEU_alias, UNM_alias, DAR_alias, NHANES_alias]\n",
    "for a in all_analytes['variable'].unique():\n",
    "    for c in cohorts:\n",
    "        t = all_analytes[all_analytes['CohortType']==c]\n",
    "        t = t[t['variable']==a]\n",
    "        coh.append(c)\n",
    "        analyte.append(a)\n",
    "        in_range.append(len(t[t['In_LOD_Range']==1]))\n",
    "        n.append(len(t[t['In_LOD_Range']==1])+len(t[t['In_LOD_Range']==0]))\n",
    "lods_in_range_df = pd.DataFrame(data = {'Cohort' : coh, 'Analyte' : analyte, 'In_LOD_Range' : in_range, 'N' : n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e63828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lods_in_range_df['Pct_In_Range'] = round(lods_in_range_df['In_LOD_Range']/lods_in_range_df['N'],2)\n",
    "lods_in_range_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42339c3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "\n",
    "g = sns.catplot(\n",
    "    data = lods_in_range_df,\n",
    "    x = 'Cohort', y = 'Pct_In_Range',\n",
    "    col ='Analyte', kind = 'bar', col_wrap = 5, sharey = False, palette = ['red', 'green', 'gray', 'blue']\n",
    ")\n",
    "# iterate through axes and set bar label as number of datapoints\n",
    "for ax in g.axes.ravel():\n",
    "\n",
    "    # add annotations\n",
    "    for c in ax.containers:\n",
    "        labels = [f'{(v.get_height()):.2f}' for v in c]\n",
    "        ax.bar_label(c, labels=labels, label_type = 'edge')\n",
    "    ax.margins(y = 0.2)\n",
    "\n",
    "g.savefig('Percent_LODS_In_Range_Ratio_Hist_DP_Paper.jpg', format ='jpeg', dpi =1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking for PROTECT for USB\n",
    "d = all_combined_stacks_melts[all_combined_stacks_melts['CohortType']=='PROTECT']\n",
    "d = d[d['Analyte']=='USB']\n",
    "print('in range:', len(d[d['value'].between(0.001000, 0.40)]))\n",
    "print('total:', len(d))\n",
    "print('in range version 2:',len(df_NEU[df_NEU['USB'].between(0.001000, 0.4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb160d",
   "metadata": {},
   "source": [
    "**Creating Boxplots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combined_stacks_melts['variable']=all_combined_stacks_melts['variable'].apply(rename_var)\n",
    "all_combined_stacks_melts_alod['variable']=all_combined_stacks_melts_alod['variable'].apply(rename_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c21edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NHANES_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combined_stacks_melts['CohortType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4ad04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##boxplopts on analyte value\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plot_df = all_combined_stacks_melts[all_combined_stacks_melts['CohortType'].isin([NEU_alias,DAR_alias,UNM_alias, NHANES_alias])]\n",
    "g = sns.catplot(\n",
    "    data = plot_df,\n",
    "    x = 'CohortType', y ='value',\n",
    "    col ='variable', kind ='box', col_wrap = 5, sharey = False, palette =  ['red', 'green', 'gray', 'blue'])\n",
    "\n",
    "# set rotation\n",
    "g.set_xticklabels(rotation=45)\n",
    "\n",
    "g.savefig('Boxplots_DP_Paper.jpg', format = 'jpeg', dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0be192",
   "metadata": {},
   "outputs": [],
   "source": [
    "##boxplopts on analyte value\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plot_df = all_combined_stacks_melts_alod[all_combined_stacks_melts_alod['CohortType'].isin([NEU_alias,DAR_alias,UNM_alias, NHANES_alias])]\n",
    "g = sns.catplot(\n",
    "    data = plot_df,\n",
    "    x = 'CohortType', y ='value',\n",
    "    col ='variable', kind ='box', col_wrap = 5, sharey = False, palette =  ['red', 'green', 'gray', 'blue'])\n",
    "\n",
    "# set rotation\n",
    "g.set_xticklabels(rotation=45)\n",
    "\n",
    "g.savefig('ALOD_Boxplots_DP_Paper.jpg', format = 'jpeg', dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['CohortType'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8903324",
   "metadata": {},
   "source": [
    "**One-Way Anova of Geometric Means**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593c440",
   "metadata": {},
   "source": [
    "Meeting assumptions\n",
    "\n",
    "1. Normally distributed: over 30 datapoints so Central Limit Theorem says they are normally distributed\n",
    "2. Independent Groups: met by structure of data\n",
    "3. Homogeneity of Variances (equal variances): will be tested for each analyte group\n",
    "\n",
    "Source: https://online.stat.psu.edu/stat200/lesson/10/10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcf3c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "from scipy.stats import f_oneway\n",
    "from statistics import variance\n",
    "\n",
    "##get colors to print in color\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "##conduct a one way anova of all analytes\n",
    "def one_way_anova(df_all, fi_name, intersec):\n",
    "    ##define lists that report results of anova\n",
    "    analytes = []\n",
    "    assumptions = []\n",
    "    p_vals = []\n",
    "    sig = []\n",
    "    variances = []\n",
    "\n",
    "    for col in intersec:\n",
    "        if col not in ['TimePeriod', 'CohortType', 'PIN_Patient']:\n",
    "            analytes.append(col)\n",
    "            ##get series of analyte values w/out NaN\n",
    "            NEU = df_ALL[df_ALL['CohortType'] == NEU_alias][col].dropna()\n",
    "            UNM = df_ALL[df_ALL['CohortType'] == UNM_alias][col].dropna()\n",
    "            DAR = df_ALL[df_ALL['CohortType'] == DAR_alias][col].dropna()\n",
    "            NHANES = df_ALL[df_ALL['CohortType'] == NHANES_alias][col].dropna()\n",
    "            var = [round(variance(NEU),3), round(variance(DAR),3), round(variance(UNM),3), round(variance(NHANES),3)]\n",
    "            variances.append(var)\n",
    "            ##if equal variances, conduct anova\n",
    "            ##if round(variance(NEU), 2) == round(variance(UNM), 2) == round(variance(DAR), 2) == round(variance(NHANES), 2):\n",
    "            if max(var) / min(var) < 2:\n",
    "                print(bcolors.OKGREEN + col, \"passes assumptions\" + bcolors.OKGREEN, '\\n')\n",
    "                assumptions.append(True)\n",
    "\n",
    "            else:                     \n",
    "                print(bcolors.FAIL + col, \"fails: unequal variances\" + bcolors.FAIL)\n",
    "                assumptions.append(False)\n",
    "                    #perform one-way ANOVA\n",
    "            p = f_oneway(NEU , UNM ,DAR , NHANES).pvalue\n",
    "            p_vals.append(p)\n",
    "            if p > 0.05:\n",
    "                sig.append(False)\n",
    "            else:\n",
    "                sig.append(True)\n",
    "    ##make df from results of anova\n",
    "    data = {'Analytes' : analytes, 'Passes Equal Variance Assumption' : assumptions, 'P-Value' : p_vals, \n",
    "            'Significant' : sig, 'Variance' : variances}\n",
    "    res_df = pd.DataFrame(data)\n",
    "    res_df.to_csv(fi_name, index = False)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf02ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anova_df = one_way_anova(df_ALL, 'one_way_anova_results.csv', intersec_cols)\n",
    "anova_df_log = one_way_anova(df_ALL_log, 'log_one_way_anova_results.csv', intersec_cols_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_df_alod = one_way_anova(df_ALL_alod, 'ALOD_one_way_anova_results.csv', intersec_cols)\n",
    "anova_df_log_alod = one_way_anova(df_ALL_log_alod, 'ALOD_log_one_way_anova_results.csv', intersec_cols_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95dfe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "##post hoc tests on analytes that were found to be significantly different by ANOVA\n",
    "##if p < 0.05,there is a statistically significant difference in means\n",
    "def tukey(df_neu, df_dar, df_unm, df_nhanes, cols_to_test, fi_name):\n",
    "    for col in cols_to_test:\n",
    "        NEU = df_ALL[df_ALL['CohortType']==NEU_alias].dropna(subset = [col])\n",
    "        DAR = df_ALL[df_ALL['CohortType']==DAR_alias].dropna(subset = [col])\n",
    "        UNM = df_ALL[df_ALL['CohortType']==UNM_alias].dropna(subset = [col])\n",
    "        NHANES = df_ALL[df_ALL['CohortType']==NHANES_alias].dropna(subset = [col])\n",
    "        df = pd.concat([NEU, DAR, UNM, NHANES])\n",
    "        tukey = pairwise_tukeyhsd(endog = df[col],\n",
    "                                  groups = df['CohortType'],\n",
    "                                  alpha=0.05)\n",
    "        file_name = fi_name\n",
    "        try:\n",
    "            f = open(file_name, 'x')\n",
    "            f = open(file_name, 'x')\n",
    "\n",
    "        except:\n",
    "            f = open(file_name, 'a')\n",
    "            f.write(col + ' Tukey Result: \\n')\n",
    "            f.write(str(tukey))\n",
    "            f.write('\\n\\n')\n",
    "\n",
    "        f.close()\n",
    "    return print(str(tukey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a2dc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols_to_test = anova_df['Analytes']\n",
    "tukey(df_NEU, df_DAR, df_UNM, df_NHANES, cols_to_test, 'tukey_results.csv')\n",
    "\n",
    "cols_to_test_log = anova_df_log['Analytes']\n",
    "tukey(df_NEU_log, df_DAR_log, df_UNM_log, df_NHANES_log, cols_to_test_log, 'log_tukey_results.csv')\n",
    "\n",
    "cols_to_test_alod = anova_df['Analytes']\n",
    "tukey(df_NEU_alod, df_DAR_alod, df_UNM_alod, df_NHANES_alod, cols_to_test_alod, 'ALOD_tukey_results.csv')\n",
    "\n",
    "cols_to_test_log_alod = anova_df_log_alod['Analytes']\n",
    "tukey(df_NEU_log_alod, df_DAR_log_alod, df_UNM_log_alod, df_NHANES_log_alod, cols_to_test_log_alod, 'alod_log_tukey_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90a407",
   "metadata": {},
   "source": [
    "**Kolmogorov-Smirnov Test**\n",
    "\n",
    "Source: https://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\n",
    "\n",
    "Note: Has this assumption been met - \"Perhaps the most serious limitation is that the distribution must be fully specified. That is, if location, scale, and shape parameters are estimated from the data, the critical region of the K-S test is no longer valid. It typically must be determined by simulation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0110f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##determining if two samples came from the same distribution\n",
    "##determining what distribution a sample follows\n",
    "def ks(df_all, col_to_test, fi_name):\n",
    "    from scipy.stats import ks_2samp\n",
    "    cohort1 = []\n",
    "    cohort2 = []\n",
    "    analyte = []\n",
    "    p_vals = []\n",
    "    \n",
    "    for col in cols_to_test:\n",
    "        for coh1 in [NEU_alias,UNM_alias, DAR_alias, NHANES_alias]:\n",
    "            for coh2 in  [NEU_alias,UNM_alias, DAR_alias, NHANES_alias]:\n",
    "                if coh1 != coh2:\n",
    "                    ##p < 0.05, data does NOT follow that distribution\n",
    "                    ##alternative option set to two sided means null hypothesis is that d1 = d2\n",
    "                    p_val = ks_2samp(df_ALL[df_ALL['CohortType'] == coh1].dropna(subset = [col])[col],\n",
    "                                   df_ALL[df_ALL['CohortType'] == coh2].dropna(subset = [col])[col]).pvalue\n",
    "                    cohort1.append(coh1)\n",
    "                    cohort2.append(coh2)\n",
    "                    p_vals.append(p_val)\n",
    "                    analyte.append(col)\n",
    "    res_df = pd.DataFrame(data = {'Cohort1' : cohort1, 'Cohort2' : cohort2, 'Analyte' : analyte, 'P-Value' : p_vals})\n",
    "    res_df['distribution matches'] = res_df['P-Value'].map(lambda p : p > 0.05 )\n",
    "    res_df.to_csv(fi_name, index = False)\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ks = ks(df_ALL, cols_to_test, 'ks_test_results.csv')\n",
    "df_ks_log = ks(df_ALL_log, cols_to_test, 'log_ks_test_results.csv')\n",
    "\n",
    "df_ks_alod = ks(df_ALL_alod, cols_to_test_alod, 'ALOD_ks_test_results.csv')\n",
    "df_ks_log_alod = ks(df_ALL_log_alod, cols_to_test_alod, 'ALOD_log_ks_test_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
