{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonizing analytes, creating visualizations, running analysis \n",
    "\n",
    "The goal of this notebook is to find the interesected features between all 4 cohorts, create graphs visualizing the distirubtions of the analytes, and anlyze those distributions. \n",
    "\n",
    "Author: Julia Geller\n",
    "\n",
    "Last Edit: 03/06/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "This notebook explains how to to access the dataset models from django.\n",
    "The datasets are loaded from the Postgres database into pandas dataframes.\n",
    "\n",
    "To start the notebook server:\n",
    "\n",
    "```\n",
    "# Start a bash shell inside the api container\n",
    "docker-compose exec api /bin/bash\n",
    "\n",
    "# Start the jupyter notebook\n",
    "python manage.py shell_plus --notebook\n",
    "\n",
    "# Take note of the token provided and access the notebook through:\n",
    "<ip-of-the-machine>:7777/?<token>\n",
    "http://127.0.0.1:7777/?token=30c43675981e671b4a609cff470819098e274bbde415b7f5\n",
    "```\n",
    "\n",
    "This step has only to be made once as long as the jupyter-notebook keeps executing.\n",
    "Considering that the the notebook continues to run, you can access the notebook using:\n",
    "``` \n",
    "<ip-of-the-machine>:7777/?<token>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to access the database\n",
    "import os\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Data analysis tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Models available in our application\n",
    "from datasets.models import RawFlower, RawUNM, RawDAR\n",
    "from django.contrib.auth.models import User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from api import adapters\n",
    "from api import analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming NHANES from Uploaded Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31608, 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## zlatan, my issue is here. both df's dataframe is not correclty imported. There\n",
    "##Are null values for nahens_df where there should not be and \n",
    "##non 1/0 values in BLOD where there should be\n",
    "from datasets.models import RawNHANES_BIO\n",
    "nhanes_df = adapters.nhanes.get_dataframe_orig()\n",
    "nhanes_df_blod=adapters.nhanes.get_dataframe_orig_blod()\n",
    "\n",
    "nhanes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##blod mapper is a dummy function, this fucn should not need to exist bc BLOD houldonly be 1 annd 0\n",
    "def blod_mapper(n):\n",
    "    if n >0.5 and n<=1:\n",
    "        return 1\n",
    "    if n< 0.5:\n",
    "        return 0\n",
    "    else: \n",
    "        return 9999\n",
    "\n",
    "# This queries the RawNEU dataset and excludes some of the values\n",
    "df = pd.DataFrame.from_records(\n",
    "    RawNHANES_BIO.objects.\n",
    "    # exclude(Creat_Corr_Result__lt=-1000).\n",
    "    # exclude(Creat_Corr_Result__isnull=True).\n",
    "    values()\n",
    ")\n",
    "print(df.columns)\n",
    "df['Blod']=df['Blod'].fillna(9999)\n",
    "\n",
    "df['Blod']=df['Blod'].map(blod_mapper)\n",
    "df['Blod']=df['Blod'].astype(int)\n",
    "df['Blod']=df['Blod'].replace(9999, np.NaN)\n",
    "\n",
    "## new covariates\n",
    "df['Member_c'] = 1\n",
    "df.columns = ['id', 'PIN_Patient', 'Age', 'TimePeriod', 'Pregnant', 'Marital',\n",
    "   'Child_A', 'Child_B', 'H_Inc', 'F_Inc', 'Edu', 'Rac', 'BLOD',\n",
    "   'Result', 'Analyte','Member_c']\n",
    "#ga at collection\n",
    "\n",
    "# Pivoting the table and reseting index\n",
    "numerical_values = ['Result']\n",
    "\n",
    "columns_to_indexes = ['PIN_Patient', 'TimePeriod', 'Member_c' ]\n",
    "categorical_to_columns = ['BLOD']\n",
    "\n",
    "df = pd.pivot_table(df, values=numerical_values,\n",
    "                    index=columns_to_indexes,\n",
    "                    columns=categorical_to_columns)\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "\n",
    "# TODO - Should we drop NaN here?\n",
    "\n",
    "# After pivot\n",
    "# Analyte     TimePeriod Member_c       BCD  ...      UTMO       UTU       UUR\n",
    "# PIN_Patient                                ...\n",
    "# A0000M               1        1  1.877245  ...  0.315638  1.095520  0.424221\n",
    "# A0000M               3        1  1.917757  ...  0.837639  4.549155  0.067877\n",
    "# A0001M               1        1  1.458583  ...  0.514317  1.262910  1.554346\n",
    "# A0001M               3        1  1.365789  ...  0.143302  1.692582  0.020716\n",
    "# A0002M               1        1  1.547669  ...  0.387643  0.988567  1.081877\n",
    "\n",
    "df['CohortType'] = 'NHANES'\n",
    "#df['TimePeriod'] = pd.to_numeric(df['TimePeriod'], errors='coerce')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##commented out for testing purposes\n",
    "\"\"\" from datasets.models import RawNEU\n",
    "df = pd.DataFrame.from_records(\n",
    "        RawNEU.objects.\n",
    "        # exclude(Creat_Corr_Result__lt=-1000).\n",
    "        # exclude(Creat_Corr_Result__isnull=True).\n",
    "        values()\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#adapters.neu.get_dataframe()\n",
    "neu_df = adapters.neu.get_dataframe()\n",
    "neu_df_blod=adapters.neu.get_dataframe_BLOD()\n",
    "\n",
    "##making mock df\n",
    "for col in ['UBA','UBE', 'UCD', 'UCO', 'UCR', 'UCS', 'UCU', 'UHG', 'UMN', 'UMO', 'UNI',\n",
    "            'UPB', 'UPT', 'USB', 'USE', 'USN', 'UTAS', 'UTL', 'UTU', 'UUR', 'UVA',\n",
    "            'UZN']:\n",
    "    neu_df_blod[col]=np.random.randint(0,2,size=(1, 2200))[0]\n",
    "\n",
    "##unm_df = adapters.unm.get_dataframe_orig()\n",
    "##unm_df_blod=adapters.unm.get_dataframe_orig_BLOD()\n",
    "\n",
    "unm_df=neu_df\n",
    "unm_df_blod=neu_df_blod\n",
    "\n",
    "##DELETE once have real dar data\n",
    "dar_df=neu_df\n",
    "dar_df_blod=neu_df_blod\n",
    "\n",
    "##dar_df = adapters.dar.get_dataframe()\n",
    "####dar_df_blod = adapters.dar.get_dataframe_BLOD()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhanes_df_blod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_df_blod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersected Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##returns list of biometric samples column names present in df \n",
    "list_notin=['PIN_Patient', 'TimePeriod', 'Member_c', 'Outcome', 'Outcome_weeks',\n",
    "       'age', 'ethnicity', 'race', 'education', 'BMI', 'income', 'smoking',\n",
    "       'parity', 'preg_complications', 'folic_acid_supp', 'fish', 'babySex',\n",
    "       'birthWt', 'headCirc', 'birthLen', 'WeightCentile', 'LGA', 'SGA',\n",
    "       'ga_collection', 'creatininemgdl_x', 'birth_year', 'CohortType', 'original',\n",
    "       'prediction', 'prediction_xvalue', 'original_xvalue',\n",
    "       'creatininemgdl_y', 'zscore', 'Cohort', 'dil_indicator','fish_pu_v2','SPECIFICGRAVITY_V2_x',\n",
    "            'SPECIFICGRAVITY_V2_y' ]\n",
    "def cat_samples_list(df):\n",
    "    samples=[]\n",
    "    for col in df.columns:\n",
    "        if col not in list_notin:\n",
    "            samples.append(col)\n",
    "    return samples\n",
    "neu_anlaytes_list = cat_samples_list(neu_df)\n",
    "unm_anlaytes_list = cat_samples_list(unm_df)\n",
    "dar_anlaytes_list = cat_samples_list(dar_df)\n",
    "nhanes_anlaytes_list = cat_samples_list(nhanes_df)\n",
    "\n",
    "neu_unm = set.intersection(set(neu_anlaytes_list), set(unm_anlaytes_list))\n",
    "neu_unm_dar = list(set.intersection(set(neu_unm), set(dar_anlaytes_list)))\n",
    "all_cohorts = list(set.intersection(set(neu_unm_dar), set(nhanes_anlaytes_list)))\n",
    "\n",
    "len(all_cohorts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_cohorts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DF with intersected features\n",
    "\n",
    "##### Can you also rewrite the function to see what is harmonized across two cohorts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'PIN_Patient' not in all_cohorts:\n",
    "    all_cohorts.insert(0,\"PIN_Patient\")\n",
    "\n",
    " \n",
    "print(\"Cohort(s) : Total Data Size (for interescted features)\")\n",
    "\n",
    "neu_intersected=neu_df[all_cohorts]\n",
    "neu_intersected[\"CohortType\"]=\"NEU\"\n",
    "neu_size=neu_intersected.shape[0]\n",
    "print(\"NEU : \" +format(neu_size, \"0.0f\"))\n",
    "\n",
    "\n",
    "unm_intersected=unm_df[all_cohorts]\n",
    "unm_intersected[\"CohortType\"]=\"UNM\"\n",
    "unm_size=unm_intersected.shape[0]\n",
    "print(\"UNM : \" + format(unm_size,\"0.0f\"))\n",
    "\n",
    "dar_intersected=dar_df[all_cohorts]\n",
    "dar_intersected[\"CohortType\"]=\"DAR\"\n",
    "dar_size=dar_intersected.shape[0]\n",
    "print(\"DAR : \" + format(unm_size,\"0.0f\"))\n",
    "\n",
    "nhanes_intersected=nhanes_df[all_cohorts]\n",
    "nhanes_intersected[\"CohortType\"]=\"NHANES\"\n",
    "nhanes_size=nhanes_intersected.shape[0]\n",
    "print(\"NHANES : \" + format(nhanes_size,\"0.0f\"))\n",
    "\n",
    "if 'CohortType' not in all_cohorts:\n",
    "    all_cohorts.insert(0,\"CohortType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "       'age', 'ethnicity', 'race', 'BMI', 'smoking', 'parity',\n",
    "       'preg_complications', 'folic_acid_supp', 'fish', 'babySex', 'birthWt',\n",
    "       'birthLen', 'headCirc', 'WeightCentile', 'LGA', 'SGA', 'ga_collection',\n",
    "       'education', 'birth_year', 'SPECIFICGRAVITY_V2', 'fish_pu_v2', 'UBA',\n",
    "       'UBE', 'UCD', 'UCO', 'UCR', 'UCS', 'UCU', 'UHG', 'UMN', 'UMO', 'UNI',\n",
    "       'UPB', 'UPT', 'USB', 'USE', 'USN', 'UTAS', 'UTL', 'UTU', 'UUR', 'UVA',\n",
    "       'UZN', 'CohortType'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhanes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) \n",
    "neu_df[all_cohorts].describe().transpose()\n",
    "\n",
    "##truncates column values\n",
    "def clean_cols(DF_desc):\n",
    "    DF_desc[\"count\"]=DF_desc[\"count\"].astype(int)\n",
    "    DF_desc[\"mean\"]=round(DF_desc[\"mean\"],2)\n",
    "    DF_desc[\"std\"]=round(DF_desc[\"std\"],2)\n",
    "    DF_desc[\"min\"]=round(DF_desc[\"min\"],2)\n",
    "    DF_desc[\"25%\"]=round(DF_desc[\"25%\"],2)\n",
    "    DF_desc[\"50%\"]=round(DF_desc[\"50%\"],2)\n",
    "    DF_desc[\"75%\"]=round(DF_desc[\"75%\"],2)\n",
    "    DF_desc[\"max\"]=round(DF_desc[\"max\"],2)\n",
    "\n",
    "\n",
    "\n",
    "# NEU\n",
    "NEU_desc=neu_df[all_cohorts].describe().transpose()\n",
    "clean_cols(NEU_desc)\n",
    "\n",
    "# DAR - dummy\n",
    "DAR_desc=dar_df[all_cohorts].describe().transpose()\n",
    "clean_cols(DAR_desc)\n",
    "\n",
    "# UNM - dummy\n",
    "UNM_desc=unm_df[all_cohorts].describe().transpose()\n",
    "clean_cols(UNM_desc)\n",
    "\n",
    "# NHANES - dummy\n",
    "NHANES_desc=nhanes_df[all_cohorts].describe().transpose()\n",
    "clean_cols(NHANES_desc)\n",
    "\n",
    "# Write functions that will iterate and save each summary dataframe to a csv file.\n",
    "frames_names = [(NEU_desc,\"NEU\"), (DAR_desc, \"DAR\"),(UNM_desc,\"UNM\"),(NHANES_desc,\"NHANES\")]\n",
    "\n",
    "file_name = \"Summary_Stats_Distribution_Paper_JGeller\"\n",
    "\n",
    "\n",
    "try:\n",
    "    f = open(file_name+\".csv\", \"x\")\n",
    "    f = open(file_name+\".csv\", \"a\")\n",
    "    \n",
    "except:\n",
    "    f = open(file_name+\".csv\", \"a\")\n",
    "    \n",
    "for tup in frames_names:\n",
    "    frame=tup[0]\n",
    "    name=tup[1]\n",
    "    ##content=string(name ,\"/n\",frame,\"\\n\\n\")\n",
    "    f.write(name)\n",
    "    f.write(\"\\n\\t\")\n",
    "    f.write(str(frame))\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "f.close()\n",
    "\n",
    "frames_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#2) TODO: Create a report of the counts per analyte provided by each cohort\n",
    "# plot the counts using a this using a histogram\n",
    "\n",
    "##build dictionary with values corresponding to each column\n",
    "vals={}\n",
    "for col in all_cohorts:\n",
    "    if col==\"PIN_Patient\":\n",
    "        None\n",
    "    else:\n",
    "        vals[col]=[]\n",
    "\n",
    "##report counts for each data frame and add values to dictionary\n",
    "for tup in frames_names:\n",
    "    i=0\n",
    "    name=tup[1]\n",
    "    print('\\033[1m'+name+'\\033[0m')\n",
    "    vals[\"CohortType\"].append(name)\n",
    "\n",
    "    while i<len(tup[0]):\n",
    "        feature=tup[0].index[i]\n",
    "        feature_count=tup[0][\"count\"][i]\n",
    "        print(\"   \"+feature+\": \"+format(feature_count,\"0.0f\"))\n",
    "        i=i+1\n",
    "        vals[feature].append(feature_count)\n",
    "\n",
    "\n",
    "        ##=tup[0][\"count\"][i]\n",
    "##create dataframe fro values in dictionary\n",
    "counts=pd.DataFrame(columns=list(vals.keys()), data=vals)\n",
    "counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "# cool feature for data frames if you want to make the dataframe also show a gradient\n",
    "counts_df=counts.style.background_gradient(cmap=cm)\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib histogram \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn histogram - finalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages\n",
    "- Clean\n",
    "- Simple\n",
    "- Don't manually add features\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "counts_melted=pd.melt(counts, id_vars = ['CohortType'])\n",
    "\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=counts_melted,\n",
    "    x='CohortType', y='value',\n",
    "    col='variable', kind='bar', col_wrap=5, sharey = False,       \n",
    "    palette=sns.color_palette(['black', 'green','gray']))\n",
    "\n",
    "\n",
    "##JAG TODO understand what is happening here\n",
    "# iterate through axes\n",
    "for ax in g.axes.ravel():\n",
    "    \n",
    "    # add annotations\n",
    "    for c in ax.containers:\n",
    "        labels = [f'{(v.get_height()):.0f}' for v in c]\n",
    "        ax.bar_label(c, labels=labels, label_type='edge')\n",
    "    ax.margins(y=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3) TODO: Generate boxplot data to visualize individual and combinations of data\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "##creating dataframe - need to melt eaach combination of df, and then put them into one df\n",
    "##for below boxplot\n",
    "\n",
    "neu_ints_melt = pd.melt(neu_intersected_cols.drop('PIN_Patient', axis = 1), id_vars=\"CohortType\")\n",
    "if \"Analyte\" in neu_ints_melt.columns:\n",
    "    neu_ints_melt=neu_ints_melt.rename(columns={\"Analyte\": \"variable\"})\n",
    "    \n",
    "dar_ints_melt=pd.melt(dar_intersected_cols.drop('PIN_Patient', axis = 1), id_vars=\"CohortType\")\n",
    "if \"Analyte\" in dar_ints_melt.columns:\n",
    "    dar_ints_melt=dar_ints_melt.rename(columns={\"Analyte\": \"variable\"})\n",
    "    \n",
    "unm_ints_melt = pd.melt(unm_intersected_cols.drop('PIN_Patient', axis = 1), id_vars=\"CohortType\")\n",
    "if \"Analyte\" in unm_ints_melt.columns:\n",
    "    unm_ints_melt=unm_ints_melt.rename(columns={\"Analyte\": \"variable\"})\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "all_combined_stacks_melts=pd.concat([neu_ints_melt,unm_ints_melt,dar_ints_melt], axis=0)\n",
    "\n",
    "print(\"Checking that melted df has same correct number of values per cohorts\")\n",
    "print(\"Cohort values: \", all_combined_stacks_melts[\"CohortType\"].unique())\n",
    "print(all_combined_stacks_melts[all_combined_stacks_melts[\"CohortType\"]==\"NEU\"].count()[0],\"=\", neu_size*(len(intersec_3_cohs_list)-2))\n",
    "print(all_combined_stacks_melts[all_combined_stacks_melts[\"CohortType\"]==\"DAR\"].count()[0],\"=\", dar_size*(len(intersec_3_cohs_list)-2))\n",
    "print(all_combined_stacks_melts[all_combined_stacks_melts[\"CohortType\"]==\"UNM\"].count()[0],\"=\", unm_size*(len(intersec_3_cohs_list)-2))\n",
    "                                                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "plot_df = all_combined_stacks_melts[all_combined_stacks_melts['CohortType'].isin(['NEU','DAR', 'UNM'])]\n",
    "g = sns.catplot(\n",
    "    data=plot_df,\n",
    "    x='CohortType', y='value',\n",
    "    col='variable', kind='box', col_wrap=5, sharey = False,\n",
    "    palette=sns.color_palette(['black', 'green','gray']), order=['NEU','DAR', 'UNM']\n",
    ")\n",
    "\n",
    "# set rotation\n",
    "g.set_xticklabels(rotation=90)\n",
    "\n",
    "ann_text <- data.frame(mpg = 15,wt = 5,lab = \"Text\",\n",
    "                       cyl = factor(8,levels = c(\"4\",\"6\",\"8\")))\n",
    "p + geom_text(data = ann_text,label = \"Text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neu_uasb=plot_df[plot_df[\"CohortType\"]==\"NEU\"]\n",
    "neu_uasb=neu_uasb[neu_uasb[\"variable\"]==\"UASB\"][\"value\"]\n",
    "\n",
    "unm_uasb=plot_df[plot_df[\"CohortType\"]==\"UNM\"]\n",
    "unm_uasb=unm_uasb[unm_uasb[\"variable\"]==\"UASB\"][\"value\"]\n",
    "\n",
    "dar_uasb=plot_df[plot_df[\"CohortType\"]==\"DAR\"]\n",
    "dar_uasb=dar_uasb[dar_uasb[\"variable\"]==\"UASB\"][\"value\"]\n",
    "\n",
    "data = [neu_uasb,dar_uasb, unm_uasb]\n",
    "fig7, ax7 = plt.subplots()\n",
    "ax7.set_title('Multiple Samples with Different sizes')\n",
    "ax7.boxplot(data)\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# Create two subplots and unpack the output array immediately\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.plot(x, y)\n",
    "ax1.set_title('Sharing Y axis')\n",
    "ax2.scatter(x, y)\n",
    "\n",
    "# Create four polar axes and access them through the returned array\n",
    "fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n",
    "axs[0, 0].plot(x, y)\n",
    "axs[1, 1].scatter(x, y)\n",
    "\n",
    "# Share a X axis with each column of subplots\n",
    "plt.subplots(2, 2, sharex='col')\n",
    "\n",
    "# Share a Y axis with each row of subplots\n",
    "plt.subplots(2, 2, sharey='row')\n",
    "\n",
    "# Share both X and Y axes with all subplots\n",
    "plt.subplots(2, 2, sharex='all', sharey='all')\n",
    "\n",
    "# Note that this is the same as\n",
    "plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "# Create figure number 10 with a single subplot\n",
    "# and clears it if it already exists.\n",
    "fig, ax = plt.subplots(num=10, clear=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "neu_uasb=plot_df[plot_df[\"CohortType\"]==\"NEU\"]\n",
    "neu_uasb=neu_uasb[neu_uasb[\"variable\"]==\"UASB\"][\"value\"]\n",
    "\n",
    "unm_uasb=plot_df[plot_df[\"CohortType\"]==\"UNM\"]\n",
    "unm_uasb=unm_uasb[unm_uasb[\"variable\"]==\"UASB\"][\"value\"]\n",
    "\n",
    "dar_uasb=plot_df[plot_df[\"CohortType\"]==\"DAR\"]\n",
    "dar_uasb=dar_uasb[dar_uasb[\"variable\"]==\"UASB\"][\"value\"]\n",
    "\n",
    "data = [neu_uasb,dar_uasb, unm_uasb]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "features=intersec_3_cohs_list[2:]\n",
    "fig, axs = plt.subplots(math.ceil(math.sqrt(i)), math.ceil(math.sqrt(i)))\n",
    "i=1\n",
    "r=0\n",
    "c=0\n",
    "while i<=len(features):\n",
    "    while r<=math.ceil(math.sqrt(i)):\n",
    "        while c<=math.ceil(math.sqrt(i)):\n",
    "            axs[r, c].boxplot(data)\n",
    "            axs[r, c].set_title('Axis ['+ str(r)+ \" , \" +str(c) +']')\n",
    "            c=c+1\n",
    "            print(str(r), str(c))\n",
    "        r=r+1\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='x-label', ylabel='y-label')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "medians = plot_df.groupby(['CohortType',\"variable\"])['value'].count()\n",
    "vertical_offset = plot_df['value'].median() * 0.05 # offset from median for display\n",
    "print(medians)\n",
    "for xtick in box_plot.get_xticks():\n",
    "    box_plot.text(xtick,medians[xtick] + vertical_offset,medians[xtick], \n",
    "            horizontalalignment='center',size='x-small',color='w',weight='semibold')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4) TODO: Generate a visualization also where you overlay the three density plots for each cohort over each other\n",
    "# If you can add vertical lines in the distribution plot to show the mean.\n",
    "\n",
    "g = sns.FacetGrid(plot_df, col='variable', hue='CohortType', col_wrap = 4, sharex = False,palette=sns.color_palette(['black', 'gray','green']))\n",
    "p1 = g.map(sns.kdeplot, 'value').add_legend()\n",
    "\n",
    "\n",
    "#example:\n",
    "#https://stackoverflow.com/questions/41144357/showing-the-mean-line-in-a-density-plot-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5) TODO: Correlate the 15 harmonized urine values but write your own correlation functions and visualizaions. \n",
    "##heatmap of correlation in each cohort \n",
    "import seaborn as sns\n",
    "# Generate correlations for datasets:\n",
    "# NEU\n",
    "# DAR\n",
    "# UNM\n",
    "\n",
    "features=intersec_3_cohs_list[2:]\n",
    "\n",
    "fig, ax =plt.subplots(1,3)\n",
    "fig.set_size_inches(17, 5)\n",
    "\n",
    "sns.heatmap(neu_df[intersec_3_cohs_list[2:]].corr(), ax=ax[0])\n",
    "ax[0].set_title('NEU')\n",
    "##ax[0].set_xticklabels(labels=features,rotation=45, fontsize=5)\n",
    "##ax[0].set_yticklabels(labels=features,rotation=0)\n",
    "\n",
    "sns.heatmap(dar_df[intersec_3_cohs_list[2:]].corr(), ax=ax[1])\n",
    "ax[1].set_title('DAR')\n",
    "##ax[1].set_xticklabels(labels=features,rotation=45)\n",
    "##ax[1].set_yticklabels(labels=features,rotation=0)\n",
    "\n",
    "sns.heatmap(unm_df[intersec_3_cohs_list[2:]].corr(), ax=ax[2])\n",
    "ax[2].set_title('UNM')\n",
    "##ax[2].set_xticklabels(labels=features,rotation=45)\n",
    "##ax[2].set_yticklabels(labels=features,rotation=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOD Graphs - Plotting number of detects (0) per Analyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating BLOD df \n",
    "BLOD=pd.DataFrame(columns=[\"CohortType\"], data=[\"NEU\", \"DAR\", \"UNM\"])\n",
    "\n",
    "##intesected columns iwth just bio-sample columns\n",
    "copy_intersec=intersec_3_cohs_list\n",
    "if \"PIN_Patient\" in copy_intersec:\n",
    "    copy_intersec.remove(\"PIN_Patient\")\n",
    "if \"CohortType\" in copy_intersec:\n",
    "    copy_intersec.remove(\"CohortType\")\n",
    "\n",
    "##filling df with number of 0's per each analyte per cohort\n",
    "for col in copy_intersec:\n",
    "    BLOD[col]=[neu_df_blod[neu_df_blod[col]==0].count()[0],\n",
    "                  dar_df_blod[dar_df_blod[col]==0].count()[0],\n",
    "                 dar_df_blod[dar_df_blod[col]==0].count()[0]]\n",
    "BLOD_melted=pd.melt(BLOD, id_vars = ['CohortType'])\n",
    "\n",
    "##creating barchart from the counts\n",
    "g = sns.catplot(\n",
    "    data=BLOD_melted,\n",
    "    x='CohortType', y='value',\n",
    "    col='variable', kind='bar', col_wrap=5, sharey = False,       \n",
    "    palette=sns.color_palette(['black', 'green','gray']))\n",
    "\n",
    "##adding label with is number of 0's per analyte\n",
    "for ax in g.axes.ravel():\n",
    "    \n",
    "    # add annotations\n",
    "    for c in ax.containers:\n",
    "        labels = [f'{(v.get_height()):.0f}' for v in c]\n",
    "        ax.bar_label(c, labels=labels, label_type='edge')\n",
    "    ax.margins(y=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) TODO: Can you write about what you see. Are there any cohorts that are similar with the harmonized data? \n",
    "# Are there any analytes that are particularliy higher lower in specific cohorts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) TODO: Think about if how we could compare similarity between two cohorts?\n",
    "\n",
    "- Look at the shape of the distributions\n",
    "- Look at peak of distributions (mode)\n",
    "- Look at median and compare to dashed line (mean)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
